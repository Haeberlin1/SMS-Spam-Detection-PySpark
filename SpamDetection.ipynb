{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW6XzAJBCO9r",
        "outputId": "f9fecb02-f323-4da9-a35c-6338d7d71b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "rAMlB8VQCkB3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SMS_SpamDetection').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNEZcQvMNI-r",
        "outputId": "10ab9e53-290b-464c-e40f-e37f9dfdc542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Import"
      ],
      "metadata": {
        "id": "Zm2sMO7A9En6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "JAP7hnu4DHws"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/230109 - SMS_SpamDetection/spam.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "L69LT6YSC4Dj"
      },
      "outputs": [],
      "source": [
        "data_original = spark.read.csv(path, inferSchema=True,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOSslrTzDd1s",
        "outputId": "c5ebe179-4140-4eb8-cdef-88491541f0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- v1: string (nullable = true)\n",
            " |-- v2: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_original.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct75aLlDDkjm",
        "outputId": "8172c55b-df86-45aa-f1a9-204dceb8ef79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v1 : ham\n",
            "v2 : U dun say so early hor... U c already then say...\n",
            "_c2 : None\n",
            "_c3 : None\n",
            "_c4 : None\n"
          ]
        }
      ],
      "source": [
        "for keys,item in zip(data_original.head(5)[3].asDict().keys(),\n",
        "                     data_original.head(5)[3].asDict().values()):\n",
        "  print(keys+' : '+str(item))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOwiQbBmDyoq",
        "outputId": "15cdacd0-97a6-4342-ff78-105a71bc52a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+----+----+----+\n",
            "| v1| v2| _c2| _c3| _c4|\n",
            "+---+---+----+----+----+\n",
            "|  0|  1|5525|5562|5568|\n",
            "+---+---+----+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "\n",
        "data_original.select(\n",
        "    [\n",
        "        count(\n",
        "            when(\n",
        "                isnan(c) | col(c).isNull(), c\n",
        "                 )\n",
        "            ).alias(c) for c in data_original.columns\n",
        "     ]\n",
        "     ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnl3ryE1D5G0"
      },
      "source": [
        "The cols _c2,_c3 and _c4 have no information and can be dropped.\n",
        "\n",
        "For clarity, columns will be renamed. Also, a columns with the length of the message will be created, rows with null values will be dropped and the filter will guarantee that the class is consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "8m0MdTxeD4w2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import length,mean\n",
        "\n",
        "data = data_original.select('v1','v2')\\\n",
        "        .withColumnRenamed('v1','class')\\\n",
        "        .withColumn('length',length(data_original['v2']))\\\n",
        "        .withColumnRenamed('v2','message')\\\n",
        "        .na.drop()\\\n",
        "        .filter(\n",
        "                (data_original['v1'] == 'ham') |\\\n",
        "                (data_original['v1'] == 'spam')\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc8dCWa_FNad",
        "outputId": "8a64d7df-c49b-494e-e3a2-e62389d6d6f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+------+\n",
            "|class|             message|length|\n",
            "+-----+--------------------+------+\n",
            "|  ham|Go until jurong p...|   111|\n",
            "|  ham|Ok lar... Joking ...|    29|\n",
            "+-----+--------------------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5av9_LKwJsgn",
        "outputId": "7c5a3f10-f414-4080-e0ef-e71a2acc2234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+------------------+\n",
            "|class|count(class)|       avg(length)|\n",
            "+-----+------------+------------------+\n",
            "|  ham|        4825| 71.07357512953368|\n",
            "| spam|         747|138.45917001338688|\n",
            "+-----+------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.groupBy('class')\\\n",
        "    .agg(\n",
        "        count('class'),\n",
        "         mean('length')\n",
        "         ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2garW06OHnB"
      },
      "source": [
        "It looks like ham messages tend to be shorter, on average, than spam messages. Therefore, the interest in the message length is justified and can help the model to differentiate between good message and SPAM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Modeling"
      ],
      "metadata": {
        "id": "YbIlsryEdlUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "nOSZKWgtQlx-"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import (RegexTokenizer,StopWordsRemover, \n",
        "                                CountVectorizer,IDF,\n",
        "                                StringIndexer,VectorAssembler)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classes must be numerical values. So, we will use StringIndexer."
      ],
      "metadata": {
        "id": "0n7BdfQZejcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = StringIndexer(\n",
        "                        inputCol=\"class\",\n",
        "                        outputCol=\"label\",\n",
        "                        stringOrderType=\"frequencyDesc\"\n",
        "                        )\n",
        "\n",
        "index_data = indexer.fit(data).transform(data)\n",
        "\n",
        "index_data.groupBy('label')\\\n",
        "    .agg(\n",
        "        count('label'),\n",
        "         mean('length')\n",
        "         ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7XkQxVVdsUU",
        "outputId": "3ccb4627-032f-4de3-b303-60540269c015"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+------------------+\n",
            "|label|count(label)|       avg(length)|\n",
            "+-----+------------+------------------+\n",
            "|  0.0|        4825| 71.07357512953368|\n",
            "|  1.0|         747|138.45917001338688|\n",
            "+-----+------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to transform the col message into a vector column containing the words. Spaces have to be removed."
      ],
      "metadata": {
        "id": "5s6mr5r6ewjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = RegexTokenizer(\n",
        "                        inputCol=\"message\",\n",
        "                        outputCol=\"words\",\n",
        "                        pattern=\"\\\\W\"\n",
        "                        )\n",
        "token_data = token.transform(index_data)\n",
        "\n",
        "token_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX9BbGDZe_di",
        "outputId": "0a28aeaa-ab86-453b-e96d-dd5c909616d3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+------+-----+--------------------+\n",
            "|class|             message|length|label|               words|\n",
            "+-----+--------------------+------+-----+--------------------+\n",
            "|  ham|Go until jurong p...|   111|  0.0|[go, until, juron...|\n",
            "|  ham|Ok lar... Joking ...|    29|  0.0|[ok, lar, joking,...|\n",
            "| spam|Free entry in 2 a...|   155|  1.0|[free, entry, in,...|\n",
            "|  ham|U dun say so earl...|    49|  0.0|[u, dun, say, so,...|\n",
            "|  ham|Nah I don't think...|    61|  0.0|[nah, i, don, t, ...|\n",
            "+-----+--------------------+------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some words do not convey information. They are called \"stop words\". Spark has a fuction to remove them."
      ],
      "metadata": {
        "id": "rQ7cI3xLftzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop = StopWordsRemover(inputCol='words', outputCol='n_words')\n",
        "\n",
        "stop_data = stop.transform(token_data)\n",
        "\n",
        "stop_data.select('words','n_words').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0pkGuZgf5U-",
        "outputId": "6e8358aa-f37f-4a43-9dde-0b41dd55cdf1"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               words|             n_words|\n",
            "+--------------------+--------------------+\n",
            "|[go, until, juron...|[go, jurong, poin...|\n",
            "|[ok, lar, joking,...|[ok, lar, joking,...|\n",
            "|[free, entry, in,...|[free, entry, 2, ...|\n",
            "|[u, dun, say, so,...|[u, dun, say, ear...|\n",
            "|[nah, i, don, t, ...|[nah, think, goes...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning techniques usually only accept numeric inputs. CountVectorizer is a Spark function that creates a numeric dictionary that relates all the words contained in the messages and their relative (to the row) frequencies.   "
      ],
      "metadata": {
        "id": "IRxOEV_sj1fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "countvec = CountVectorizer(inputCol='n_words',outputCol='cvec_words')\n",
        "\n",
        "countvec_data = countvec.fit(stop_data).transform(stop_data)\n",
        "\n",
        "countvec_data.select('cvec_words').show(1,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njv80_kYuVH0",
        "outputId": "393578b6-81be-49c4-ef12-14a1db74aa81"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|cvec_words                                                                                                                                |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(8537,[11,16,37,62,69,79,249,544,632,770,1252,1295,1308,2873,4668,4907],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF - IDF (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
      ],
      "metadata": {
        "id": "mC_24KlElWUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idf_model = IDF(inputCol='cvec_words',outputCol='idf_words')\n",
        "\n",
        "idf_data = idf_model.fit(countvec_data).transform(countvec_data)\n",
        "\n",
        "idf_data.select('idf_words').show(1,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUhfBeLkvUW0",
        "outputId": "cfaa9c6c-b2b1-466c-f768-8cf4f696dc4b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|idf_words                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(8537,[11,16,37,62,69,79,249,544,632,770,1252,1295,1308,2873,4668,4907],[3.0535347553917718,3.1919667840152965,3.781501701110945,3.9343409053403926,3.9909597993399006,4.159780668914952,5.042169849113426,5.681249808403096,5.917638586467326,5.986631457954278,6.546247245889701,6.546247245889701,6.546247245889701,7.527076498901427,7.932541607009591,7.932541607009591])|\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the data has to be framed in a format acceptable for Spark. All the features or dependent variables must be put in a single column. Each row contains a vector that condenses all the features in a single element. Usually, this columns is called 'features'. This is done easily with VectorAssmebler.\n",
        "\n",
        "Another column with the labels in numeric format is necessary. The custom is to call it 'label'."
      ],
      "metadata": {
        "id": "EOOJByhtlsKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=['idf_words','length'],outputCol='features')\n",
        "\n",
        "treated_data = assembler.transform(idf_data)\n",
        "\n",
        "treated_data.select('features','label').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXPUAnswwrvn",
        "outputId": "e6d0a1a0-afd0-4261-d0f9-8187ea894550"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(8538,[11,16,37,6...|  0.0|\n",
            "|(8538,[0,9,244,36...|  0.0|\n",
            "|(8538,[2,10,23,24...|  1.0|\n",
            "|(8538,[0,57,85,86...|  0.0|\n",
            "|(8538,[53,136,366...|  0.0|\n",
            "|(8538,[9,15,21,26...|  1.0|\n",
            "|(8538,[15,132,286...|  0.0|\n",
            "|(8538,[149,157,31...|  0.0|\n",
            "|(8538,[1,64,82,14...|  1.0|\n",
            "|(8538,[0,1,10,31,...|  1.0|\n",
            "|(8538,[3,22,29,33...|  0.0|\n",
            "|(8538,[6,17,21,24...|  1.0|\n",
            "|(8538,[10,24,26,5...|  1.0|\n",
            "|(8538,[45,77,84,1...|  0.0|\n",
            "|(8538,[479,677,85...|  0.0|\n",
            "|(8538,[24,37,80,1...|  1.0|\n",
            "|(8538,[3,41,63,27...|  0.0|\n",
            "|(8538,[0,2,71,73,...|  0.0|\n",
            "|(8538,[0,72,91,13...|  0.0|\n",
            "|(8538,[5,24,26,42...|  1.0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treated_data.select('features')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikhkaSvol_6H",
        "outputId": "b203dfb4-0597-4386-8da5-b5e79c22336a"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[features: vector]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "JiXKKdQHogtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the processes performed in the previous section are usually done in a condensed form with a pipeline. The main advatange is the reuse of all the steps in a single command. In this example, the model is left out of the pipeline, but it could be inserted here as well."
      ],
      "metadata": {
        "id": "kdTyVut8oi5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "wqJ7wpYVo6yV"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_prep_data = Pipeline(stages=[indexer, token, stop, countvec,\n",
        "                                  idf_model, assembler])"
      ],
      "metadata": {
        "id": "hh8yIqcxpIQv"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_data = pipe_prep_data.fit(data).transform(data)"
      ],
      "metadata": {
        "id": "uBJrJTOipIW_"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_data.select('features','label').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyWtOu9NqIDl",
        "outputId": "70d2c4b1-b77a-4e87-dcbe-536cabfc522e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(8538,[11,16,37,6...|  0.0|\n",
            "|(8538,[0,9,244,36...|  0.0|\n",
            "|(8538,[2,10,23,24...|  1.0|\n",
            "|(8538,[0,57,85,86...|  0.0|\n",
            "|(8538,[53,136,366...|  0.0|\n",
            "|(8538,[9,15,21,26...|  1.0|\n",
            "|(8538,[15,132,286...|  0.0|\n",
            "|(8538,[149,157,31...|  0.0|\n",
            "|(8538,[1,64,82,14...|  1.0|\n",
            "|(8538,[0,1,10,31,...|  1.0|\n",
            "|(8538,[3,22,29,33...|  0.0|\n",
            "|(8538,[6,17,21,24...|  1.0|\n",
            "|(8538,[10,24,26,5...|  1.0|\n",
            "|(8538,[45,77,84,1...|  0.0|\n",
            "|(8538,[479,677,85...|  0.0|\n",
            "|(8538,[24,37,80,1...|  1.0|\n",
            "|(8538,[3,41,63,27...|  0.0|\n",
            "|(8538,[0,2,71,73,...|  0.0|\n",
            "|(8538,[0,72,91,13...|  0.0|\n",
            "|(8538,[5,24,26,42...|  1.0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can compare treated_data and prep_data and conclude that they are exactly the same."
      ],
      "metadata": {
        "id": "-pbcw0dAqMjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Modelling"
      ],
      "metadata": {
        "id": "Hcwss9ywrM8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test Split"
      ],
      "metadata": {
        "id": "GxZ4NeKOxKUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common way to create and test a model is to split the data in train and test dataframes."
      ],
      "metadata": {
        "id": "3ryDPPtKyEwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data = prep_data.randomSplit([0.8,0.2],seed = 1)"
      ],
      "metadata": {
        "id": "HBTiNyIXyQvi"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.groupBy('label').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LdHjWIXziWt",
        "outputId": "f691892e-8993-4e72-9bc2-5a5b178302cc"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0| 3832|\n",
            "|  1.0|  594|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.groupBy('label').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7dMn8IazdMe",
        "outputId": "f1f0e455-2d2b-453b-a976-534399f5eea9"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0|  993|\n",
            "|  1.0|  153|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can remark that both datasets maintained the same level of label imbalance."
      ],
      "metadata": {
        "id": "JgwfPZJFzvK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "upHh3E_yrUH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is classically used for Natural Language Classification. It relies on the estimation of conditional probabilities. Intuitevely, it checks on how much the probability of an element to be one class or the other increases due to the presence of each element in the message. Unfortunately this probabilities can only be estimated and not calculated accurately.\n",
        "\n",
        "This approach fits well with the problem at hand. For example, suppose the word \"order\" or \"buy\" appears in a message. We can not know for sure that it is a SPAM just based on that, but we can say it gets more probable. Combinig this with other words and their conditional probabilities is what the Naive Bayes attempts to do."
      ],
      "metadata": {
        "id": "uxb1zAqErRgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes"
      ],
      "metadata": {
        "id": "CQ-Mawjkvzct"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nB = NaiveBayes(featuresCol='features',\n",
        "                      labelCol='label',\n",
        "                      modelType='multinomial')"
      ],
      "metadata": {
        "id": "Y-nNKgi5wL8Y"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nB_model = nB.fit(train_data)"
      ],
      "metadata": {
        "id": "Gko6M2WiwuIg"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = nB_model.transform(test_data)\n",
        "test_results.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFzLgVy7y36o",
        "outputId": "ecbab3fe-a01b-4663-b191-8ac77baa92c9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['class',\n",
              " 'message',\n",
              " 'length',\n",
              " 'label',\n",
              " 'words',\n",
              " 'n_words',\n",
              " 'cvec_words',\n",
              " 'idf_words',\n",
              " 'features',\n",
              " 'rawPrediction',\n",
              " 'probability',\n",
              " 'prediction']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_results.select('label','prediction','rawPrediction','probability').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv8o3ZcIzImb",
        "outputId": "631eac1a-b9f7-4538-9e5c-5412a530ab42"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+--------------------+--------------------+\n",
            "|label|prediction|       rawPrediction|         probability|\n",
            "+-----+----------+--------------------+--------------------+\n",
            "|  0.0|       0.0|[-859.65131284924...|[1.0,4.3097634138...|\n",
            "|  0.0|       0.0|[-4650.7280415482...|[1.0,7.7828248558...|\n",
            "|  0.0|       0.0|[-4650.7280415482...|[1.0,7.7828248558...|\n",
            "|  0.0|       0.0|[-1203.6600955646...|[1.0,1.2878081462...|\n",
            "|  0.0|       0.0|[-1411.3269760801...|[1.0,1.3678122809...|\n",
            "|  0.0|       0.0|[-1220.8401401176...|[1.0,1.3227880678...|\n",
            "|  0.0|       0.0|[-480.14388362531...|[1.0,6.1770067477...|\n",
            "|  0.0|       0.0|[-292.17748613866...|[1.0,1.3484351044...|\n",
            "|  0.0|       0.0|[-426.76108379898...|[1.0,3.6604115341...|\n",
            "|  0.0|       0.0|[-1905.6818024263...|[1.0,3.6097431012...|\n",
            "|  0.0|       0.0|[-1674.1179745284...|[0.95938665094168...|\n",
            "|  0.0|       0.0|[-227.60994623319...|[0.99999978380961...|\n",
            "|  0.0|       0.0|[-463.80407128968...|[1.0,1.8051991557...|\n",
            "|  0.0|       0.0|[-376.25638955003...|[1.0,2.5197814669...|\n",
            "|  0.0|       0.0|[-378.86442935953...|[1.0,5.5895255672...|\n",
            "|  0.0|       0.0|[-889.18198660483...|[1.0,2.6488788870...|\n",
            "|  0.0|       0.0|[-36.208228311163...|[0.99999985361701...|\n",
            "|  0.0|       0.0|[-112.93466403114...|[1.0,2.9161312375...|\n",
            "|  0.0|       1.0|[-117.25768279759...|[0.01669187093660...|\n",
            "|  0.0|       1.0|[-1035.8304505799...|[0.44306626932948...|\n",
            "+-----+----------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ConfusionMatrixStats(test_results): \n",
        "\n",
        "    #True Positives\n",
        "    TP = test_results.filter((test_results['prediction']==1) & (test_results['label'] == 1)).count()\n",
        "    #True Negatives\n",
        "    TN = test_results.filter((test_results['prediction']==0) & (test_results['label'] == 0)).count()\n",
        "    #False Positives\n",
        "    FP = test_results.filter((test_results['prediction']==1) & (test_results['label'] == 0)).count()\n",
        "    #False Negatives\n",
        "    FN = test_results.filter((test_results['prediction']==0) & (test_results['label'] == 1)).count()\n",
        "\n",
        "    mat = [[TP,FN],[FP,TN]]\n",
        "    print('-----------------------------------------------')\n",
        "    print('TP  =   {}         FN  =   {}   '.format(TP,FN))\n",
        "    print('FP  =   {}         TN  =   {}   '.format(FP,TN))\n",
        "\n",
        "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
        "    print('The accuracy for the test dataset is {}.'.format(accuracy))\n",
        "    #recall or true positive rate\n",
        "    recall = TP/(TP+FN)\n",
        "    print('The recall for the test dataset is {}.'.format(recall))\n",
        "    #negative recall or true negative rate\n",
        "    selec = TN/(FP+TN)\n",
        "    print('The selectivity for the test dataset is {}.'.format(selec))\n",
        "    #precision or positive predicted value\n",
        "    prec = TP/(FP+TP)\n",
        "    print('The precision for the test dataset is {}.'.format(prec))\n"
      ],
      "metadata": {
        "id": "68Fv3Fqe0F7G"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixStats(test_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQmvtJGY12No",
        "outputId": "f1594f29-b5a9-4709-88db-a8184c50c14b"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "TP  =   151         FN  =   2   \n",
            "FP  =   49         TN  =   944   \n",
            "The accuracy for the test dataset is 0.9554973821989529.\n",
            "The recall for the test dataset is 0.9869281045751634.\n",
            "The selectivity for the test dataset is 0.9506545820745217.\n",
            "The precision for the test dataset is 0.755.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "b-QbWXTU5N0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the Naive Bayes model with a Random Forest."
      ],
      "metadata": {
        "id": "jvpXm_-u5Rye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier as RFC"
      ],
      "metadata": {
        "id": "eP61K-oT5aBf"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_model = RFC(\n",
        "                featuresCol='features',\n",
        "                labelCol='label',\n",
        "                maxDepth=10,\n",
        "                numTrees=200,\n",
        "                impurity='entropy')\n",
        "\n",
        "rfc_test_results = rfc_model.fit(train_data).transform(test_data)\n",
        "ConfusionMatrixStats(rfc_test_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-rxay9y5pqL",
        "outputId": "6abc4310-229f-48a0-c0ea-1aac76197970"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "TP  =   26         FN  =   127   \n",
            "FP  =   0         TN  =   993   \n",
            "The accuracy for the test dataset is 0.8891797556719022.\n",
            "The recall for the test dataset is 0.16993464052287582.\n",
            "The selectivity for the test dataset is 1.0.\n",
            "The precision for the test dataset is 1.0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest model was much inferior to the Naive Bayes. It learned just the Ham class and was not able to learn the Spam class, since most of positives were missed. It almost guessed every instance to be Ham. The threshold to determine an instance as SPAM became too high."
      ],
      "metadata": {
        "id": "eizLpVo670bj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}